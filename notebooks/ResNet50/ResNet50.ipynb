{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import io\n",
    "import shutil"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_HEIGHT, IMG_WIDTH = (224, 224)\n",
    "PREPROCESS_SEED = 123\n",
    "CHECKPOINTS_DIR = Path(\"checkpoints\")\n",
    "LOGS_DIR = Path(\"logs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "base_data_dir = Path(\"..\", \"..\", \"input\", \"arch-recognizer-dataset\").absolute()\n",
    "val_data_dir = base_data_dir / \"val\"\n",
    "test_data_dir = base_data_dir / \"test\"\n",
    "train_data_dir = base_data_dir / \"train\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# # Import data\n",
    "\n",
    "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     val_data_dir,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"int\",\n",
    "#     seed=PREPROCESS_SEED,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     crop_to_aspect_ratio=True,\n",
    "# )\n",
    "\n",
    "# test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     test_data_dir,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"int\",\n",
    "#     seed=PREPROCESS_SEED,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     crop_to_aspect_ratio=True,\n",
    "# )\n",
    "\n",
    "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     train_data_dir,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"int\",\n",
    "#     seed=PREPROCESS_SEED,\n",
    "#     image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     crop_to_aspect_ratio=True,\n",
    "# )\n",
    "\n",
    "# class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "# def preprocess(img):\n",
    "#     # tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)(img)\n",
    "#     # tf.keras.applications.resnet.preprocess_input(img)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# train_ds.map(lambda img, _: preprocess(img))\n",
    "# val_ds.map(lambda img, _: preprocess(img))\n",
    "# test_ds.map(lambda img, _: preprocess(img))\n",
    "\n",
    "# train_ds = train_ds.shuffle(5000).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# val_ds = val_ds.shuffle(5000).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# test_ds = test_ds.shuffle(5000).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Define functions to generate confusion matrix\n",
    "\n",
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "        class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(len(class_names), len(class_names)))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    return figure"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Define function for restoring checkpoints \n",
    "def restore_weights_from_checkpoint(model):\n",
    "    latest_cp = tf.train.latest_checkpoint(CHECKPOINTS_DIR)\n",
    "    if latest_cp:\n",
    "        model.load_weights(latest_cp)\n",
    "        _, restored_test_acc = model.evaluate(test_ds, verbose=2)\n",
    "        print(f\"Restored model test accuracy: {restored_test_acc}\")\n",
    "    return model\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Configure Hyperparameters\n",
    "\n",
    "CNN_APPS = {\n",
    "    tf.keras.applications.VGG19.__name__: {\n",
    "        \"image_size\": (224, 224),\n",
    "        \"scale\": 1.0 / 255,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.vgg19.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.ResNet50V2.__name__: {\n",
    "        \"image_size\": None,\n",
    "        \"scale\": 1.0 / 255,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.resnet.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.ResNet152V2.__name__: {\n",
    "        \"image_size\": None,\n",
    "        \"scale\": 1.0 / 255,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.resnet.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.InceptionV3.__name__: {\n",
    "        \"image_size\": None,\n",
    "        \"scale\": 1.0,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.inception_v3.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.InceptionResNetV2.__name__: {\n",
    "        \"image_size\": None,\n",
    "        \"scale\": 1.0,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.MobileNetV2.__name__: {\n",
    "        \"image_size\": (224, 224),\n",
    "        \"scale\": 1.0,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.DenseNet201.__name__: {\n",
    "        \"image_size\": (224, 224),\n",
    "        \"scale\": 1.0 / 255,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.densenet.preprocess_input,\n",
    "    },\n",
    "    tf.keras.applications.EfficientNetB7.__name__: {\n",
    "        \"image_size\": None,\n",
    "        \"scale\": 1.0 / 255,\n",
    "        \"offset\": 0,\n",
    "        \"preprocessor\": tf.keras.applications.efficientnet.preprocess_input,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "HP_CNN_MODEL = hp.HParam(\"model\", hp.Discrete(list(CNN_APPS.keys())))\n",
    "HP_WEIGHTS = hp.HParam(\"weights\", hp.Discrete([\"\", \"imagenet\"]))\n",
    "HP_LEARNING_RATE = hp.HParam(\n",
    "    \"learning_rate\", hp.Discrete([float(1e-4), float(3e-4), float(5e-4)])\n",
    ")\n",
    "\n",
    "METRIC_ACCURACY = \"accuracy\"\n",
    "\n",
    "with tf.summary.create_file_writer(f\"logs/hparam_tuning\").as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_CNN_MODEL, HP_WEIGHTS, HP_LEARNING_RATE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name=\"Test Accuracy\")],\n",
    "    )\n",
    "\n",
    "# runs = []\n",
    "# run_num = 0\n",
    "# for cnn_model in HP_CNN_MODEL.domain.values:\n",
    "#     for weights in HP_WEIGHTS.domain.values:\n",
    "#         for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "#             runs.append(\n",
    "#                 {\n",
    "#                     \"name\": f\"run-{run_num}-{cnn_model}-{weights}-{learning_rate}\",\n",
    "#                     HP_CNN_MODEL: cnn_model,\n",
    "#                     HP_WEIGHTS: weights,\n",
    "#                     HP_LEARNING_RATE: learning_rate,\n",
    "#                 }\n",
    "#             )\n",
    "#             run_num += 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define training run routine\n",
    "# Import data\n",
    "\n",
    "\n",
    "def get_datasets(img_height, img_width):\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train_data_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        seed=PREPROCESS_SEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        val_data_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        seed=PREPROCESS_SEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "    )\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        test_data_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        seed=PREPROCESS_SEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "    )\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def run(hparams, run_name):\n",
    "    cnn_app = CNN_APPS[hparams[HP_CNN_MODEL]]\n",
    "\n",
    "    train_ds, val_ds, test_ds = get_datasets(*cnn_app[\"image_size\"])\n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    train_ds.map(lambda img, _: cnn_app[\"preprocessor\"](img))\n",
    "    val_ds.map(lambda img, _: cnn_app[\"preprocessor\"](img))\n",
    "    test_ds.map(lambda img, _: cnn_app[\"preprocessor\"](img))\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    def get_cnn_model(hparams):\n",
    "        _weights = hparams[HP_WEIGHTS] if hparams[HP_WEIGHTS] else None\n",
    "        _classes = len(class_names) if not hparams[HP_WEIGHTS] else None\n",
    "        return {\n",
    "            tf.keras.applications.VGG19.__name__: tf.keras.applications.VGG19(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.ResNet50V2.__name__: tf.keras.applications.ResNet50V2(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.ResNet152V2.__name__: tf.keras.applications.ResNet152V2(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.InceptionV3.__name__: tf.keras.applications.InceptionV3(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.InceptionResNetV2.__name__: tf.keras.applications.InceptionResNetV2(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.MobileNetV2.__name__: tf.keras.applications.MobileNetV2(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.DenseNet201.__name__: tf.keras.applications.DenseNet201(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "            tf.keras.applications.EfficientNetB7.__name__: tf.keras.applications.EfficientNetB7(\n",
    "                include_top=True, weights=_weights, classes=_classes\n",
    "            ),\n",
    "        }[hparams[HP_CNN_MODEL]]\n",
    "\n",
    "    model = restore_weights_from_checkpoint(\n",
    "        tf.keras.models.Sequential(\n",
    "            [\n",
    "                # Augmentation\n",
    "                tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "                    \"horizontal\", input_shape=(*cnn_app[\"image_size\"], 3)\n",
    "                ),\n",
    "                tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "                # Convolution\n",
    "                get_cnn_model(hparams),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[METRIC_ACCURACY],\n",
    "    )\n",
    "\n",
    "    # Defining a file writer for confusion matrix logging\n",
    "    cm_file_writer = tf.summary.create_file_writer(str(LOGS_DIR / \"cm\"))\n",
    "\n",
    "    def log_confusion_matrix(epoch, logs):\n",
    "        pred_y, true_y = [], []\n",
    "        for batch_X, batch_y in test_ds:\n",
    "            true_y.extend(batch_y)\n",
    "            pred_y.extend(np.argmax(model.predict(batch_X), axis=-1))\n",
    "        cm_data = np.nan_to_num(sklearn.metrics.confusion_matrix(true_y, pred_y))\n",
    "        cm_figure = plot_confusion_matrix(cm_data, class_names=class_names)\n",
    "        cm_image = plot_to_image(cm_figure)\n",
    "        with cm_file_writer.as_default():\n",
    "            tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        # epochs=100,\n",
    "        epochs=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=LOGS_DIR / run_name, histogram_freq=1, profile_batch=1\n",
    "            ),\n",
    "            tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=str(CHECKPOINTS_DIR)\n",
    "                + f\"/{run_name}\"\n",
    "                + \"-epoch-{epoch:04d}.ckpt\",\n",
    "                monitor=METRIC_ACCURACY,\n",
    "                verbose=1,\n",
    "                save_best_only=True,\n",
    "                save_freq=\"epoch\",\n",
    "            ),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                min_delta=0.0001, patience=10, restore_best_weights=True\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    _, accuracy = model.evaluate(test_ds)\n",
    "    return accuracy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Execute training\n",
    "\n",
    "# If no saved checkpoints, reset logs by deleting the logs dir\n",
    "if not CHECKPOINTS_DIR.exists() or not list(CHECKPOINTS_DIR.iterdir()):\n",
    "    shutil.rmtree(LOGS_DIR)\n",
    "\n",
    "# # Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all --reload_interval 10\n",
    "\n",
    "# Perform training runs\n",
    "run_num = 0\n",
    "for cnn_model in HP_CNN_MODEL.domain.values:\n",
    "    for weights in HP_WEIGHTS.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams = {\n",
    "                HP_CNN_MODEL: cnn_model,\n",
    "                HP_WEIGHTS: weights,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "            }\n",
    "            run_name = f\"run-{run_num}-{cnn_model}-{weights}-{learning_rate}\"\n",
    "            run_logs_file_writer = tf.summary.create_file_writer(logdir=str(LOGS_DIR / run_name))\n",
    "            print(f\"--- Starting training run_num\")\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            with run_logs_file_writer.as_default():\n",
    "                hp.hparams(hparams)  # record the values used in this run\n",
    "                test_accuracy = run(hparams, run_name)\n",
    "                tf.summary.scalar(METRIC_ACCURACY, test_accuracy, step=1)\n",
    "            run_num += 1\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1375216), started 0:46:25 ago. (Use '!kill 1375216' to kill it.)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5ae6782cd98eceed\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5ae6782cd98eceed\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Starting training run_num\n",
      "{'model': 'DenseNet201', 'weights': '', 'learning_rate': 0.0001}\n",
      "Found 7080 files belonging to 25 classes.\n",
      "Found 2021 files belonging to 25 classes.\n",
      "Found 1012 files belonging to 25 classes.\n",
      " 27/443 [>.............................] - ETA: 4:12 - loss: 3.2479 - accuracy: 0.0880"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf5dc9dec0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrun_logs_file_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# record the values used in this run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRIC_ACCURACY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mrun_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-370ff8e07050>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(hparams, run_name)\u001b[0m\n\u001b[1;32m    134\u001b[0m             ),\n\u001b[1;32m    135\u001b[0m             tf.keras.callbacks.EarlyStopping(\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             ),\n\u001b[1;32m    138\u001b[0m         ],\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}